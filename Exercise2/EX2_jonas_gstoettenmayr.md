# Exercise 2 - Jonas Gstöttenmayr

To see the code: https://github.com/jonas-gstoettenmayr/SDB3_2/tree/main/Exercise2

## Activitiy 1

**Considering the above part Debezium CDC with PostgreSQL and Kafka, explain with your own words what it does and why it is a relevant software architecture for Big Data in the AI era and for which use cases.**

We run both Kafka and postgress as well as a connnector.

Postgress is for the permament data storage.
There is not much more to say about our postgress, as it is a normal relational DB running on one machine.
Kafka is the messaging layer.
The connector, Debezium CDC, monitors our DB and whenevery a change happens sends a message to the Kafka topic.
If we listen to the Kafka topic we can react to the event of a change in the DB.

The heart of the architecture here is the messaging layer Kafka while we don't have fixed consumers yet, other then the short examples ones durring the lecture it is still set up to be ready to be queried.

Our Kafka implimentation has one topic where we post events to. Said topic is split into 4 partitions, meaning it stores the data on 4 different machines (normally currently it's running locally so it just uses 4 cors). This partitioning speeds up the writing and reading as it can be heavily parallelised.

It is relevant as we can very quickly "ask" Kafka topics to give us specific data and also send it to KAFKA for further processing before it is permanently stored in a DB.

It is important because KAFKA is an immutable log of events, in this case DB modifications, whith high parallelisation and thus performance.

Kafka though only keeps the Data stored for a limited period of time so we need to move it to permanent storage before it's expiration date. It can also be configured to fetch data from permanent storage allowing it to truly act as a "simple" interface for all our data needs, either storing, processing or even fetching.

The use case it simple the moving/processing/storing of large data it can be used anywhere where there are large amount of data being produced and needing to be stored/processed. And I do mean large large, otherwise the cost of operating Kafka can easily outway the benefit (though it can be used for smaller scale the complexity is questionable of the usefullness in these scenarios.)

## Activity 2

### A2 - Part 1

**In a simple use case where sensor readings need to be processed every 10 minutes to calculate the average temperature over that time window, describe which software architecture would be most appropriate for fetching the data from PostgreSQL, and explain the rationale behind your choice.**

Monolith with a DB, running in docker on our lockal office server.

We can even use docker swarm, which would be a bit overkill but it could automatically restart the servie if it failed, though I would not recommend that. Better yet we simply have a batch script that queries if the service running our script is still alive and restarts it if that is not the case.

As the volume, variety and velocity are very low, the best solution is most often the simplest one.
Simply store and query the PostgressSQL DB, no need for anything fancy.
This can all be done with one application or script in this case.

This solution will make debuging very easy and with the velocity and volumne that we won't have to worry for scaling for quite some time.
Any more complexity would simply cost more compute power and thus also money.

### A2 - Part 2

**From the architectural choice made in Part 1, implement the solution to consume and processing the data generated by the temperature_data_producer.py file (revise its features!). The basic logic from the file temperature_data_consumer.py should be extended with the conection to data source defined in Part 1's architecture..**

See the "temperature_data_consumer.py".

```python
result = []
with psycopg.connect(
    dbname=DB_NAME,  # default DB
    user=DB_USER,
    password=DB_PASSWORD,
    host=DB_HOST,
    port=DB_PORT
) as conn:
    cursor = conn.cursor()
    cursor.execute(sql.SQL(f"SELECT * FROM {DB_TABLE} ORDER BY id DESC LIMIT 10") )
    result = cursor.fetchall()
    cursor.close()
if result is None:
    print("No Data")

temps = [x[2] for x in result]
avg_temp = sum(temps)/len(temps)
```

### A2 - Part 3

**Discuss the proposed architecture in terms of resource efficiency, operability, and deployment complexity. This includes analyzing how well the system utilizes compute, memory, and storage resources; how easily it can be operated, monitored, and debugged in production.**

The implimentation is as resource efficient as it can get.
It simply queries the DB for the 10 last entries.

It is quite operable as it simply needs to be started and if it were to actually crash restarted.

The deployment complextiy is minimal as we can simple put it into a container and start the container.

The application outputs to the console so simply checking if the output is not an error is enough for the monitoring, we can even modify the script to send a message in case of an error.

## Activiy 3

### A3 - Part 1

**Describe which software architecture would be most appropriate for fetching the data from PostgreSQL and generate alerts in real-time. Explain the rationale behind your choice.**

We want an architecture that allows for instant access to our data, as quickly and reliably as possible. This is where kafka as a messaging service shines.

As such we want a OLTP database running postgress monitorited by CDC for changes sending events to Kafka messaging layer, which is then read by microservices to detect for fraud (at this size probably running kubernetes), in the meantine the data is processes (put into correct format) and inserted into our datawarehouse, I assume the financial transactions are quite structured so a more normal DB would be fine for the warehouse over a data lake.

To repeat myself we get data into our OLTP DB where a Debezium CDC monitors changes in the DB and sends events to our Kafka topic, the amount of partitions depends on how powerfull our machines are but going with a few more is quite important here as we can't allow any down time for a system like this. As such we also want to replicate the data among the Kafka partitions. Then we can have a consumer group read and monitor it for fraud while a differen consumer process and permanently stores the data. As fraud detection requires real time data as well as a history we proply need our kafka topics to hold at least a week of transactions so as to properly see the spending patterns.

As mentioned before we want to orchestrate and load balance this system so most likly kubernetes running in cloud infrastructure to allow for rapid up/down scaling.

### A3 - part 2

**From the architectural choice made in Part 1, implement the 'consumer' to fetch and process the records generated by the fraud_data_producer.py file (revise its features!). The basic logic from the files fraud_consumer_agent1.py.py and fraud_consumer_agent2.py.py should be extended with the conection to data source defined in Part 1's architecture.**

Preparing by configuring CDC,
this also creates the topic I will be listening too.

```bash
curl -i -X POST   -H "Accept:application/json"   -H "Content-Type:application/json"   localhost:8083/connectors/   -d '{
    "name": "transactions-connector",
    "config": {
      "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
      "tasks.max": "1",
      "database.hostname": "postgres",
      "database.port": "5432",
      "database.user": "postgres",
      "database.password": "postgrespw",
      "database.dbname": "mydb",
      "table.include.list": "public.transactions",
      "slot.name": "debeziumactivity",
      "topic.prefix": "dbserver1",
      "plugin.name": "pgoutput",
      "decimal.handling.mode": "string"
    }
  }'
```

As mentioned before every update to the DB will create an event. This event is in JSON format, our scripts each have their own group (so they both read messages without interfering without eachother).

The consumers read all past messages to get enough data about past transactions to judge the user behaviour.

```python
consumer = KafkaConsumer(
    'dbserver1.public.transactions',  # topic name
    bootstrap_servers=['127.0.0.1:9094'],  # Kafka server
    auto_offset_reset='earliest',  # Starting from beginning
    enable_auto_commit=False,
    group_id='fraud-consumer-group',  # Consumer group ID
    request_timeout_ms=30_000,
    connections_max_idle_ms=45_000,
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))  # Parse JSON
)
```

### A3 - part 3

**Discuss the proposed architecture in terms of resource efficiency, operability, maintainability, deployment complexity, and overall performance and scalability. This includes discussing how well the system utilizes compute, memory, and storage resources; how easily it can be operated, monitored, and debugged in production; how maintainable and evolvable the individual components are over time; the effort required to deploy and manage the infrastructure; and the system’s ability to sustain increasing data volumes, higher ingestion rates, and a growing number of fraud detection agents without degradation of latency or reliability.**

The resource efficiency heavily depends on how we will configure kafka, we can of course ensure almost perfect uptime but it will require many broker and much replicaiton which will be quite expensive, but for a banking application that is a must, so bad resource efficency it is. With frequent replication on many broke it is especially bad on storage resources.

The operability should have little problem with this as well, while more complicated than the simple monolith if planed properly we can simply restart any crashed microservice (or rather kubernetes does it automatically for us) and the DB schema should not change in the near future. For Kafka as we have set on safty there will be no challange in keeping it operable either as all it requires is enough brokers to be online. Monitoring should not be anymore of a problem as we can simply monitor the Kafka messages or even create a loggin/monitoring topic which can monitor Kafka messages and how healty they are, kubernetes can also do frequent health checks on pods.

The maintainability should of course be excelent as we seperated our concerns and used a proper microservice architecture and not a distributed monolith allowing for smaller specialiset teams to work on their target (i.e. fraud detection).

The deplyoment complexity has increased from from the simple previous example but is not insurmountable with Kafka, as we simply always need to query Kafka while it manages all the many brokers in the background we have a simple interface and fixed address to get/put our data. The DB will be deplyed on a fixed point too but we won't really interact with it anyway beyond setting up the connections with Kafka. Most complex would be the kubernetes deployment of the differing fraud consumer aggents, and their refactoring into beenig able to actually work in parallel (well Kafka groups already manage it kinoff with each group only reading messages it hasn't already read).

The model is highly scalable and will have great performance as it can be distributed and replicated as often as needed. Even the DB does not need to be singular, we can simply spin up more instances to have the ability to write to multiple OLTP DBs before they send it all to centralized Kafka and our Datawarehouse in the end as such once deployed in the cloud and handled by an orchestrator even a enourmess spike in transactions should not be able to inhibit our system.

### A4 - part 4

**Compare the proposed architecture to Exercise 3 from previous lecture where the data from PostgreSQL was loaded to Spark (as a consumer) using the JDBC connector. Discuss both approaches at least in terms of performance, resource efficiency, and deployment complexity.**

My architecture: Kafka + Microservies

Would be much faster and more resource efficent with a lower deplyoment complexity.
As it is event driven meaning it always uses the newst data instantly instead of in micro-batches like spark.
For resource efficiency it Kafka uses cheaper disks over the more expensive RAM that spark usees.
The deployment is rather easy for the consumer as they can simply be part of the code.
So it can run in simple containers unlike spark which requires a cluster/engine to run.
Also Kafka does not use the JVM so that alone makes it better :D

Spark

A bit slower and more resource dependant with higher deplyoment complexity.

But it is much better for complex mathematicall queries so instead of simply fetching data and processing it in microservices we could process the data with spark where it is stored and simply move the results saving on network traffik.
Which would save on time and resources leading it to be a bit better than my architecture in that regard, but as it is now it would not be.
Spark also has good ML integration as such if we start using models for fraud detection beyong just the simple scripts it might be of benefit to switch to spark.